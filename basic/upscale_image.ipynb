{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import patchify\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "import glob as glob\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from model.srcnn import SRCNN\n",
    "# from model.sr_resnet import SRResNet\n",
    "import math\n",
    "from torchvision.utils import save_image\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 1\n",
    "SHOW_PATCHES = True\n",
    "STRIDE = 14\n",
    "SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install patchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_patches(patches):\n",
    "    plt.figure(figsize=(patches.shape[0], patches.shape[1]))\n",
    "    gs = gridspec.GridSpec(patches.shape[0], patches.shape[1])\n",
    "    gs.update(wspace=0.01, hspace=0.02)\n",
    "    counter = 0\n",
    "    for i in range(patches.shape[0]):\n",
    "        for j in range(patches.shape[1]):\n",
    "            ax = plt.subplot(gs[counter])\n",
    "            plt.imshow(patches[i, j, 0, :, :, :])\n",
    "            plt.axis('off')\n",
    "            counter += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(label, outputs, max_val=1.):\n",
    "    \n",
    "    label = label.cpu().detach().numpy()\n",
    "    outputs = outputs.cpu().detach().numpy()\n",
    "    diff = outputs - label\n",
    "    rmse = math.sqrt(np.mean((diff) ** 2))\n",
    "    if rmse == 0:\n",
    "        return 100\n",
    "    else:\n",
    "        PSNR = 20 * math.log10(max_val / rmse)\n",
    "        return PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(train_loss, val_loss, train_psnr, val_psnr):\n",
    "    # Loss plots.\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(train_loss, color='orange', label='train loss')\n",
    "    plt.plot(val_loss, color='red', label='validataion loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('/media/hero/Study/User/Study/data/model_out/image_out/loss.png')\n",
    "    plt.close()\n",
    "    # PSNR plots.\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(train_psnr, color='green', label='train PSNR dB')\n",
    "    plt.plot(val_psnr, color='blue', label='validataion PSNR dB')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('PSNR (dB)')\n",
    "    plt.legend()\n",
    "    plt.savefig('/media/hero/Study/User/Study/data/model_out/image_out/psnr.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_state(model, name_model = \"model\"):\n",
    "    # save the model to disk\n",
    "    print('Saving model...')\n",
    "    torch.save(model.state_dict(), f'/media/hero/Study/User/Study/data/model_out/image_out/{name_model}.pth')\n",
    "def save_model(epochs, model, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Function to save the trained model to disk.\n",
    "    \"\"\"\n",
    "    # Remove the last model checkpoint if present.\n",
    "    torch.save({\n",
    "                'epoch': epochs+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,\n",
    "                }, f\"/media/hero/Study/User/Study/data/model_out/image_out/model_ckpt.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_validation_results(outputs, epoch, batch_iter):\n",
    "    \"\"\"\n",
    "    Function to save the validation reconstructed images.\n",
    "    \"\"\"\n",
    "    save_image(\n",
    "        outputs, \n",
    "        f\"/media/hero/Study/User/Study/data/model_out/image_out/val_sr_{epoch}_{batch_iter}.png\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tạo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patches(\n",
    "    input_paths, out_hr_path, out_lr_path,\n",
    "):\n",
    "    os.makedirs(out_hr_path, exist_ok=True)\n",
    "    os.makedirs(out_lr_path, exist_ok=True)\n",
    "    all_paths = os.listdir(input_paths)\n",
    "    # for input_path in input_paths:\n",
    "    #     all_paths.extend(glob.glob(f\"{input_path}/*\"))\n",
    "    print(f\"Creating patches for {len(all_paths)} images\")\n",
    "    for image_name in tqdm(all_paths, total=len(all_paths)):\n",
    "        image_path = os.path.join(input_paths+\"/\", image_name)\n",
    "        # print(image_path)\n",
    "        # image = Image.open(image_path)\n",
    "        image = cv2.imread(image_path)\n",
    "        image_name = image_name.replace(\".png\", \"\")\n",
    "        w, h = image.shape[:2]\n",
    "        # Create patches of size (32, 32, 3)\n",
    "        patches = patchify.patchify(np.array(image), (64, 64, 3), STRIDE)\n",
    "\n",
    "        counter = 0\n",
    "        for i in range(patches.shape[0]):\n",
    "            for j in range(patches.shape[1]):\n",
    "                counter += 1\n",
    "                patch = patches[i, j, 0, :, :, :]\n",
    "                patch = cv2.cvtColor(patch, cv2.COLOR_RGB2BGR)\n",
    "                # print(f\"{out_hr_path}/{image_name}_{counter}.png\")\n",
    "                cv2.imwrite(\n",
    "                    f\"{out_hr_path}/{image_name}_{counter}.png\",\n",
    "                    patch\n",
    "                )\n",
    "                # Convert to bicubic and save.\n",
    "                h, w, _ = patch.shape\n",
    "                low_res_img = cv2.resize(patch, (int(w*0.25), int(h*0.25)), \n",
    "                                        interpolation=cv2.INTER_CUBIC)\n",
    "                # Now upscale using BICUBIC.\n",
    "                high_res_upscale = cv2.resize(low_res_img, (w, h), \n",
    "                                            interpolation=cv2.INTER_CUBIC)\n",
    "                cv2.imwrite(\n",
    "                    f\"{out_lr_path}/{image_name}_{counter}.png\",\n",
    "                    high_res_upscale\n",
    "                )\n",
    "    if SHOW_PATCHES:\n",
    "        show_patches(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = \"/media/hero/Study/User/Study/data/upscale_image/T91\"\n",
    "out_hr_path = \"/media/hero/Study/User/Study/data/upscale_image/t91_hr_patches\"\n",
    "out_lr_path = \"/media/hero/Study/User/Study/data/upscale_image/t91_lr_patches\"\n",
    "# create_patches(input_paths, out_hr_path, out_lr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = 0.25\n",
    "os.makedirs('/media/hero/Study/User/Study/data/upscale_image/test_bicubic_rgb_xx', exist_ok=True)\n",
    "os.makedirs('/media/hero/Study/User/Study/data/upscale_image/test_hr_xx', exist_ok=True)\n",
    "save_path_lr = '/media/hero/Study/User/Study/data/upscale_image/test_bicubic_rgb_xx'\n",
    "save_path_hr = '/media/hero/Study/User/Study/data/upscale_image/test_hr_xx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image_name in os.listdir(input_paths):\n",
    "#     image_path = os.path.join(input_paths+\"/\", image_name)\n",
    "\n",
    "#     # orig_img = Image.open(image_path)\n",
    "#     orig_img = cv2.imread(image_path)\n",
    "#     # image_name = image_name.replace(\".png\", \"\")\n",
    "#     # print(type(orig_img))\n",
    "#     w, h = orig_img.shape[:2]\n",
    "#     # print(f\"Original image dimensions: {w}, {h}\")\n",
    "#     cv2.imwrite(f\"{save_path_lr}/{image_name}\",orig_img)\n",
    "\n",
    "#     low_res_img = cv2.resize(orig_img, (int(h*scale_factor), int(w*scale_factor)))\n",
    "#     high_res_upscale = cv2.resize(low_res_img, (h, w))\n",
    "    \n",
    "#     cv2.imwrite(f\"{save_path_hr}/{image_name}\",high_res_upscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNNDataset(Dataset):\n",
    "    def __init__(self, image_paths, label_paths):\n",
    "        self.all_image_paths = glob.glob(f\"{image_paths}/*\")\n",
    "        self.all_label_paths = glob.glob(f\"{label_paths}/*\") \n",
    "        # print(len(self.all_label_paths), len(self.all_image_paths))\n",
    "    def __len__(self):\n",
    "        return (len(self.all_image_paths))\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.all_image_paths[index]).convert('RGB')\n",
    "        label = Image.open(self.all_label_paths[index]).convert('RGB')\n",
    "        image = np.array(image, dtype=np.float32)\n",
    "        label = np.array(label, dtype=np.float32)\n",
    "        # print(image.shape, label.shape)\n",
    "        image /= 255.\n",
    "        label /= 255.\n",
    "        image = image.transpose([2, 0, 1])\n",
    "        label = label.transpose([2, 0, 1])\n",
    "        return (\n",
    "            torch.tensor(image, dtype=torch.float),\n",
    "            torch.tensor(label, dtype=torch.float)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(\n",
    "    train_image_paths, train_label_paths,\n",
    "    valid_image_path, valid_label_paths\n",
    "):\n",
    "    dataset_train = SRCNNDataset(\n",
    "        train_image_paths, train_label_paths\n",
    "    )\n",
    "    dataset_valid = SRCNNDataset(\n",
    "        valid_image_path, valid_label_paths\n",
    "    )\n",
    "    return dataset_train, dataset_valid\n",
    "# Prepare the data loaders\n",
    "def get_dataloaders(dataset_train, dataset_valid):\n",
    "    train_loader = DataLoader(\n",
    "        dataset_train, \n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        dataset_valid, \n",
    "        batch_size=TEST_BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LABEL_PATHS = '/media/hero/Study/User/Study/data/upscale_image/t91_hr_patches'\n",
    "TRAN_IMAGE_PATHS = '/media/hero/Study/User/Study/data/upscale_image/t91_lr_patches'\n",
    "VALID_LABEL_PATHS = '/media/hero/Study/User/Study/data/upscale_image/test_hr_xx'\n",
    "VALID_IMAGE_PATHS = '/media/hero/Study/User/Study/data/upscale_image/test_bicubic_rgb_xx'\n",
    "SAVE_VALIDATION_RESULTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_valid = SRCNNDataset(\n",
    "#     VALID_IMAGE_PATHS, VALID_LABEL_PATHS\n",
    "# )\n",
    "# len(dataset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_valid = get_datasets(\n",
    "    TRAN_IMAGE_PATHS, TRAIN_LABEL_PATHS,\n",
    "    VALID_IMAGE_PATHS, VALID_LABEL_PATHS\n",
    ")\n",
    "train_loader, valid_loader = get_dataloaders(dataset_train, dataset_valid)\n",
    "print(f\"Training samples: {len(dataset_train)}\")\n",
    "print(f\"Validation samples: {len(dataset_valid)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train mô hình "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_psnr = 0.0\n",
    "    for bi, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        image_data = data[0].to(device)\n",
    "        label = data[1].to(device)\n",
    "        \n",
    "        # Zero grad the optimizer.\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image_data)\n",
    "        # print(outputs.shape)\n",
    "\n",
    "        loss = criterion(outputs, label)\n",
    "        # Backpropagation.\n",
    "        loss.backward()\n",
    "        # Update the parameters.\n",
    "        optimizer.step()\n",
    "        # Add loss of each item (total items in a batch = batch size).\n",
    "        running_loss += loss.item()\n",
    "        # Calculate batch psnr (once every `batch_size` iterations).\n",
    "        batch_psnr =  psnr(label, outputs)\n",
    "        running_psnr += batch_psnr\n",
    "    final_loss = running_loss/len(dataloader.dataset)\n",
    "    final_psnr = running_psnr/len(dataloader)\n",
    "    return final_loss, final_psnr\n",
    "\n",
    "def validate(model, dataloader, epoch, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_psnr = 0.0\n",
    "    with torch.no_grad():\n",
    "        for bi, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            image_data = data[0].to(device)\n",
    "            label = data[1].to(device)\n",
    "            \n",
    "            outputs = model(image_data)\n",
    "            loss = criterion(outputs, label)\n",
    "            # Add loss of each item (total items in a batch = batch size) .\n",
    "            running_loss += loss.item()\n",
    "            # Calculate batch psnr (once every `batch_size` iterations).\n",
    "            batch_psnr = psnr(label, outputs)\n",
    "            running_psnr += batch_psnr\n",
    "            # For saving the batch samples for the validation results\n",
    "            # every 500 epochs.\n",
    "            if SAVE_VALIDATION_RESULTS and (epoch % 500) == 0:\n",
    "                save_validation_results(outputs, epoch, bi)\n",
    "    final_loss = running_loss/len(dataloader.dataset)\n",
    "    final_psnr = running_psnr/len(dataloader)\n",
    "    return final_loss, final_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = 0.0003 # Learning rate.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "\n",
    "print(device)\n",
    "model = SRCNN().to(device)\n",
    "# model = SRResNet(2).to(device)\n",
    "# print(device)\n",
    "# model = SRCNN().to(device)\n",
    "model.load_state_dict(torch.load(\"/media/hero/Study/User/Study/data/model_out/image_out/model.pth\"))\n",
    "print(model.eval())\n",
    "# Optimizer.\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Loss function. \n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:45<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train PSNR: 27.019\n",
      "Val PSNR: 24.122\n",
      "Saving model...\n",
      "Epoch 63 of 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 68/127 [01:11<00:58,  1.00it/s]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "train_loss, val_loss = [], []\n",
    "train_psnr, val_psnr = [], []\n",
    "epochs = 300\n",
    "start = time.time()\n",
    "psnr_best_p = 0\n",
    "psnr_best_v = 0\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1} of {epochs}\")\n",
    "    train_epoch_loss, train_epoch_psnr = train(model, train_loader, optimizer,criterion,device)\n",
    "    val_epoch_loss, val_epoch_psnr = validate(model, valid_loader, epoch+1,criterion,device)\n",
    "\n",
    "    print(f\"Train PSNR: {train_epoch_psnr:.3f}\")\n",
    "    print(f\"Val PSNR: {val_epoch_psnr:.3f}\")\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    train_psnr.append(train_epoch_psnr)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    val_psnr.append(val_epoch_psnr)\n",
    "    if train_epoch_psnr > psnr_best_p:\n",
    "        psnr_best_p = train_epoch_psnr\n",
    "        save_model_state(model,name_model=\"best_train\")\n",
    "    \n",
    "    if val_epoch_psnr > psnr_best_v:\n",
    "        psnr_best_v = val_epoch_psnr\n",
    "        save_model_state(model,name_model=\"best_val\")\n",
    "    # Save model with all information every 100 epochs. Can be used \n",
    "    # resuming training.\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        save_model(epoch, model, optimizer, criterion)\n",
    "    # Save the model state dictionary only every epoch. Small size, \n",
    "    # can be used for inference.\n",
    "    save_model_state(model)\n",
    "    # Save the PSNR and loss plots every epoch.\n",
    "    save_plot(train_loss, val_loss, train_psnr, val_psnr)\n",
    "end = time.time()\n",
    "print(f\"Finished training in: {((end-start)/60):.3f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "# print(device)\n",
    "# model = SRCNN().to(device)\n",
    "# model.load_state_dict(torch.load(\"/media/hero/Study/User/Study/data/model_out/image_out/model.pth\"))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_test = \"/media/hero/Study/User/Project/image_processing/image_general/processed/infinity_ldv_create_a_sketch_of_a_house_under_a_forest_preceded_d26638e1-c9df-4ef5-9ae4-27cbf3d9e1aenum_3.png\"\n",
    "# image = Image.open(path_test).convert('RGB')\n",
    "# image = np.array(image, dtype=np.float32)\n",
    "# image = image/255\n",
    "# image = image.transpose([2, 0, 1])\n",
    "# image_processed = torch.tensor(image, dtype=torch.float, device=device)\n",
    "# outputs = model(image_processed)\n",
    "# out = np.array(outputs.detach().numpy().T*255, dtype=np.uint8)\n",
    "# out = out.transpose([1,0,2])\n",
    "\n",
    "# print(out.shape)\n",
    "# # print(torch.from_numpy(np.asarray(outputs.T)))\n",
    "# cv2.imwrite(\"/media/hero/Study/User/Study/data/model_out/image_video/Image_high_quality_.png\", out)\n",
    "# # cv2.imshow(\"image_high_quality\", out)\n",
    "# cv2.waitKey(0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botsort_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c233d23438bb60fd5b094f58eaeabe926c58cbec7675aa79b291e053bb5b88a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
